{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Scaled-Conjugate-Gradient-Algorithm\" data-toc-modified-id=\"Scaled-Conjugate-Gradient-Algorithm-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Scaled Conjugate Gradient Algorithm</a></span><ul class=\"toc-item\"><li><span><a href=\"#The-Scaled-Part\" data-toc-modified-id=\"The-Scaled-Part-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>The Scaled Part</a></span></li><li><span><a href=\"#The-Conjugate-Part\" data-toc-modified-id=\"The-Conjugate-Part-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>The Conjugate Part</a></span></li></ul></li><li><span><a href=\"#optimizers.py\" data-toc-modified-id=\"optimizers.py-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>optimizers.py</a></span><ul class=\"toc-item\"><li><span><a href=\"#SGD\" data-toc-modified-id=\"SGD-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>SGD</a></span></li><li><span><a href=\"#Adam\" data-toc-modified-id=\"Adam-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Adam</a></span></li><li><span><a href=\"#SCG\" data-toc-modified-id=\"SCG-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>SCG</a></span></li></ul></li><li><span><a href=\"#Demonstrations-on-Simple-Optimization-Problems\" data-toc-modified-id=\"Demonstrations-on-Simple-Optimization-Problems-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Demonstrations on Simple Optimization Problems</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\newcommand{\\xv}{\\mathbf{x}}\n",
    "\\newcommand{\\Xv}{\\mathbf{X}}\n",
    "\\newcommand{\\yv}{\\mathbf{y}}\n",
    "\\newcommand{\\Yv}{\\mathbf{Y}}\n",
    "\\newcommand{\\zv}{\\mathbf{z}}\n",
    "\\newcommand{\\av}{\\mathbf{a}}\n",
    "\\newcommand{\\Wv}{\\mathbf{W}}\n",
    "\\newcommand{\\wv}{\\mathbf{w}}\n",
    "\\newcommand{\\gv}{\\mathbf{g}}\n",
    "\\newcommand{\\Hv}{\\mathbf{H}}\n",
    "\\newcommand{\\dv}{\\mathbf{d}}\n",
    "\\newcommand{\\Vv}{\\mathbf{V}}\n",
    "\\newcommand{\\vv}{\\mathbf{v}}\n",
    "\\newcommand{\\tv}{\\mathbf{t}}\n",
    "\\newcommand{\\Tv}{\\mathbf{T}}\n",
    "\\newcommand{\\zv}{\\mathbf{z}}\n",
    "\\newcommand{\\Zv}{\\mathbf{Z}}\n",
    "\\newcommand{\\muv}{\\boldsymbol{\\mu}}\n",
    "\\newcommand{\\sigmav}{\\boldsymbol{\\sigma}}\n",
    "\\newcommand{\\phiv}{\\boldsymbol{\\phi}}\n",
    "\\newcommand{\\Phiv}{\\boldsymbol{\\Phi}}\n",
    "\\newcommand{\\Sigmav}{\\boldsymbol{\\Sigma}}\n",
    "\\newcommand{\\Lambdav}{\\boldsymbol{\\Lambda}}\n",
    "\\newcommand{\\half}{\\frac{1}{2}}\n",
    "\\newcommand{\\argmax}[1]{\\underset{#1}{\\operatorname{argmax}}}\n",
    "\\newcommand{\\argmin}[1]{\\underset{#1}{\\operatorname{argmin}}}\n",
    "\\newcommand{\\dimensionbar}[1]{\\underset{#1}{\\operatorname{|}}}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaled Conjugate Gradient Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Scaled Part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first derivative of an error function with respect to the\n",
    "parameters of your model tells you which direction in the parameter\n",
    "space to proceed to reduce the error function.  But how far do you go?\n",
    "So far we have just taken a small step by subtracting a small constant\n",
    "times the derivative from our current parameter values.\n",
    "\n",
    "If we are in the vicinity of a minimum of the error function, we could\n",
    "do what Newton did...approximate the function at the current parameter\n",
    "value with a parabola and solve for the minimum of the parabola.  Use\n",
    "this as the next guess at a good parameter value.  If the error\n",
    "function is quadratic in the parameter, then we jump to the true\n",
    "minimum immediately.\n",
    "\n",
    "How would you fit a parabola to a function at a particular value of\n",
    "$x$?  We can derive a way to do this using a truncated Taylor series\n",
    "(google that) to approximate the function about a value of $x$. See [Taylor Series approximation, newton's method and optimization](https://suzyahyah.github.io/calculus/optimization/2018/04/06/Taylor-Series-Newtons-Method.html) by Suzanna Sia and [Taylor Series at Wolfram Mathworld](https://mathworld.wolfram.com/TaylorSeries.html#:~:text=A%20Taylor%20series%20is%20a,known%20as%20a%20Maclaurin%20series) starting at equation 29.\n",
    "\n",
    "\n",
    "$$\n",
    "f(x+\\Delta x) \\approx \\hat{f}(x+\\Delta x) = f(x) + f'(x) \\Delta x + \n",
    "\\frac{1}{2} f''(x) \\Delta x^2 + \n",
    "$$\n",
    "\n",
    "Now we want to know what value of $\\Delta x$ minimizes\n",
    "$\\hat{f}(x+\\Delta x)$.  So take its derivative and set equal to zero.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{d \\hat{f}(x+\\Delta x)}{d\\Delta x} &= f'(x) + \\frac{1}{2} 2 f''(x)\n",
    "\\Delta x\\\\\n",
    "& = f'(x) + f''(x) \\Delta x\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Setting equal to zero we get\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "0 &= f'(x) + f''(x) \\Delta x\\\\\n",
    "\\Delta x &= -\\frac{f'(x)}{f''(x)}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Now we can update our guess for $x$ by adding $\\Delta x$ to it.  Then,\n",
    "fit a new parabola at the new value of $x$, calculate $\\Delta x$, and\n",
    "update $x$ again.  Actually, the last equation above does the parabola\n",
    "approximation and calculation of $\\Delta x$.\n",
    "\n",
    "Here is a simple example.  Say we want to find the minimum of\n",
    "\n",
    "$$\n",
    "f(x) = 2 x^4 + 3 x^3 + 3\n",
    "$$\n",
    "To calculate\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\Delta x &= -\\frac{f'(x)}{f''(x)}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "we need the function's first and second derivatives.  The are\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "f'(x) &= 8 x^3 + 9 x^2\\\\\n",
    "f''(x) &= 24 x^2 + 18 x\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "All together now, in python!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-29T18:47:38.686613Z",
     "start_time": "2021-07-29T18:47:38.200403Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import IPython.display as ipd  # for display and clear_output\n",
    "import time  # for sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-29T18:48:23.247702Z",
     "start_time": "2021-07-29T18:48:23.231948Z"
    }
   },
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return 2 * x**4 + 3 * x**3 + 3\n",
    "\n",
    "def df(x): \n",
    "    return 8 * x**3 + 9 * x**2\n",
    "\n",
    "def ddf(x):\n",
    "    return 24 * x**2 + 18*x\n",
    "\n",
    "def taylorf(x, dx):\n",
    "    return f(x) + df(x) * dx + 0.5 * ddf(x) * dx**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-29T18:50:16.434587Z",
     "start_time": "2021-07-29T18:50:02.904515Z"
    }
   },
   "outputs": [],
   "source": [
    "x = np.random.uniform(-2, 1)  # first guess at minimum\n",
    "\n",
    "xs = np.linspace(-2, 1, num=100)\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "dxs = np.linspace(-0.5, 0.5, num=100)\n",
    "\n",
    "for rep in range(10):\n",
    "    \n",
    "    time.sleep(1) \n",
    "    \n",
    "    plt.clf()\n",
    "    plt.plot(xs, f(xs))\n",
    "    plt.grid('on')\n",
    "    plt.plot(x + dxs, taylorf(x, dxs), 'g-', linewidth=5, alpha=0.4)\n",
    "    plt.plot(x, f(x), 'ro')\n",
    "    \n",
    "    y0, y1 = plt.ylim()\n",
    "    plt.plot([x, x], [y0, y1], 'r--')\n",
    "    \n",
    "    x = x - df(x) / ddf(x)\n",
    "    \n",
    "    plt.plot(x, f(x), 'go')\n",
    "    plt.text(x, (y0 + y1) * 0.5, f'{x:.4f}', color='r')\n",
    "    plt.legend(('$f(x)$','$\\hat{f}(x)$'))\n",
    "    \n",
    "    ipd.clear_output(wait=True)\n",
    "    ipd.display(fig)\n",
    "    \n",
    "ipd.clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This has all been for a function $f(x)$ of a single, scalar variable\n",
    "$x$.  To minimize a squared error function for a neural network, $x$\n",
    "will consist of all the weights of the neural network.  If all of the\n",
    "weights are collected into the vector $\\wv$, then the first derivative\n",
    "of the squared error function, $f$, with respect to the weight vector,\n",
    "$\\wv$, is a vector of derivatives like $\\frac{\\partial f}{\\partial\n",
    "w_{i}}$.  This is usually written as the gradient\n",
    "\n",
    "$$\n",
    "\\nabla_{\\wv} f =\n",
    "\\left (\\frac{\\partial f}{\\partial w_{1}}, \\frac{\\partial f}{\\partial w_{2}},\n",
    "\\ldots, \\frac{\\partial f}{\\partial w_{n}} \\right ).\n",
    "$$\n",
    "\n",
    "The second derivative will be $n\\times n$ matrix of values like\n",
    "$\\frac{\\partial^2 f}{\\partial w_i \\partial w_j}$, usually\n",
    "written as the Hessian\n",
    "\n",
    "$$\n",
    "\\nabla^2_{\\wv} f =\n",
    "\\begin{pmatrix}\n",
    "\\frac{\\partial^2 f}{\\partial w_1 \\partial w_1} & \n",
    "\\frac{\\partial^2 f}{\\partial w_1 \\partial w_2} & \n",
    "\\cdots\n",
    "\\frac{\\partial^2 f}{\\partial w_1 \\partial w_n}\\\\\n",
    "\\frac{\\partial^2 f}{\\partial w_2 \\partial w_1} & \n",
    "\\frac{\\partial^2 f}{\\partial w_2 \\partial w_2} & \n",
    "\\cdots\n",
    "\\frac{\\partial^2 f}{\\partial w_2 \\partial w_n}\\\\\n",
    "\\vdots \\\\\n",
    "\\frac{\\partial^2 f}{\\partial w_n \\partial w_1} & \n",
    "\\frac{\\partial^2 f}{\\partial w_n \\partial w_2} & \n",
    "\\cdots\n",
    "\\frac{\\partial^2 f}{\\partial w_n \\partial w_n}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "It is often impractical to\n",
    "construct and use the Hessian.  We\n",
    "will consider ways to approximate the product of the Hessian and a\n",
    "matrix as part of the Scaled Conjugate Gradient algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Conjugate Part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $E(\\wv)$ be the error function (mean square error over training samples) we wish to minimize by\n",
    "finding the best $\\wv$. Steepest descent will find new $\\wv$ by\n",
    "minimizing $E(\\wv)$ in successive directions $\\dv_0, \\dv_1, \\ldots$\n",
    "for which $\\dv_i^T \\dv_j = 0$ for $i \\neq j$.  In other words, the\n",
    "search directions are orthogonal to each other, resulting in a zig-zag\n",
    "pattern of steps, some of which are in the same directions.  \n",
    "\n",
    "Another problem with orthogonal directions is that forcing the second\n",
    "direction, for example, to be orthogonal to the first will not be in\n",
    "the direction of the minimum unless the error function is quadratic\n",
    "and its contours are circles.\n",
    "\n",
    "We would rather choose a new direction based on the previous ones and\n",
    "on the curvature, or second derivative, of the error function at the\n",
    "current $\\wv$.  This is the idea behind conjugate gradient methods.\n",
    "\n",
    "The Scaled Conjugate Gradient (SCG) algorithm,\n",
    "[Efficient\n",
    "Training of Feed-Forward Neural Networks, by Moller](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.50.8063&rep=rep1&type=pdf), combines conjugate gradient directions with a local, quadratic approximation to the error function and solving\n",
    "for the new value of $\\wv$ that would minimize the quadratic function.\n",
    "A number of additional steps are taken to improve the quadratic\n",
    "approximation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# optimizers.py\n",
    "\n",
    "The three optimization algorithms discussed so far,\n",
    "- Stochastic Gradient Descent (SGD),\n",
    "- Adaptive Moment Estimation (Adam), and\n",
    "- Scaled Conjugate Gradient (SCG)\n",
    "\n",
    "Here each is implemented as a function in a class named `Optimizers`.  Each function is appended to a file named `optimizers.py`.  First, we start with the top of the file that begins with the declaration of the `Optimizers` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile optimizers.py\n",
    "\n",
    "import numpy as np\n",
    "import copy\n",
    "import time\n",
    "import math\n",
    "import sys  # for sys.float_info.epsilon\n",
    "\n",
    "######################################################################\n",
    "## class Optimizers()\n",
    "######################################################################\n",
    "\n",
    "class Optimizers():\n",
    "\n",
    "    def __init__(self, all_weights):\n",
    "        '''all_weights is a vector of all of a neural networks weights concatenated into a one-dimensional vector'''\n",
    "        \n",
    "        self.all_weights = all_weights\n",
    "\n",
    "        self.sgd_initialized = False\n",
    "        self.scg_initialized = False\n",
    "        self.adam_initialized = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define the first optimization algorithm, SGD.  We include the option of using [Nesterov accelerated gradient (NAG)](https://ruder.io/optimizing-gradient-descent/index.html#nesterovacceleratedgradient).  This link is a nice summary of many other optimizers, also."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-29T18:40:53.351344Z",
     "start_time": "2021-07-29T18:40:53.299422Z"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile -a optimizers.py\n",
    "\n",
    "######################################################################\n",
    "#### sgd\n",
    "######################################################################\n",
    "\n",
    "    def sgd(self, error_f, gradient_f, fargs=[], n_epochs=100, learning_rate=0.001, verbose=True, error_convert_f=None, nesterov=False, callback_f=None):\n",
    "        '''\n",
    "        error_f: function that requires X and T as arguments (given in fargs) and returns mean squared error.\n",
    "        gradient_f: function that requires X and T as arguments (in fargs) and returns gradient of mean squared error\n",
    "                    with respect to each weight.\n",
    "        error_convert_f: function that converts the standardized error from error_f to original T units.\n",
    "        '''\n",
    "\n",
    "        if not self.sgd_initialized:\n",
    "            error_trace = []\n",
    "            self.momentum = 0.9\n",
    "            self.prev_update = 0\n",
    "            if nesterov:\n",
    "                self.all_weights_copy = np.zeros(self.all_weights.shape)\n",
    " \n",
    "        epochs_per_print = n_epochs // 10\n",
    "\n",
    "        for epoch in range(n_epochs):\n",
    "\n",
    "            error = error_f(*fargs)\n",
    "            grad = gradient_f(*fargs)\n",
    "\n",
    "            if not nesterov:\n",
    "                \n",
    "                self.prev_update = learning_rate * grad + self.momentum * self.prev_update\n",
    "                # Update all weights using -= to modify their values in-place.\n",
    "                self.all_weights -= self.prev_update\n",
    "\n",
    "            else:\n",
    "                self.all_weights_copy[:] = self.all_weights \n",
    "\n",
    "                self.all_weights -= self.momentum * self.prev_update\n",
    "                error = error_f(*fargs)\n",
    "                grad = gradient_f(*fargs)\n",
    "                self.prev_update = learning_rate * grad + self.momentum * self.prev_update\n",
    "                self.all_weights[:] = self.all_weights_copy\n",
    "                self.all_weights -= self.prev_update\n",
    "                \n",
    "            if error_convert_f:\n",
    "                error = error_convert_f(error)\n",
    "            error_trace.append(error)\n",
    "\n",
    "            if callback_f is not None:\n",
    "                callback_f(epoch)\n",
    "\n",
    "            if verbose and ((epoch + 1) % max(1, epochs_per_print) == 0):\n",
    "                print(f'sgd: Epoch {epoch+1:d} Error={error:.5f}')\n",
    "\n",
    "        return error_trace\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we add the basic [Adaptive Moment Estimation (Adam)](https://ruder.io/optimizing-gradient-descent/index.html#adam).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile -a optimizers.py\n",
    "\n",
    "######################################################################\n",
    "#### adam\n",
    "######################################################################\n",
    "\n",
    "    def adam(self, error_f, gradient_f, fargs=[], n_epochs=100, learning_rate=0.001, verbose=True,\n",
    "             error_convert_f=None, callback_f=None):\n",
    "        '''\n",
    "        error_f: function that requires X and T as arguments (given in fargs) and returns mean squared error.\n",
    "        gradient_f: function that requires X and T as arguments (in fargs) and returns gradient of mean squared error\n",
    "                    with respect to each weight.\n",
    "        error_convert_f: function that converts the standardized error from error_f to original T units.\n",
    "        '''\n",
    "\n",
    "        if not self.adam_initialized:\n",
    "            shape = self.all_weights.shape\n",
    "            # with multiple subsets (batches) of training data.\n",
    "            self.mt = np.zeros(shape)\n",
    "            self.vt = np.zeros(shape)\n",
    "            self.sqrt = np.sqrt\n",
    "                \n",
    "            self.beta1 = 0.9\n",
    "            self.beta2 = 0.999\n",
    "            self.beta1t = 1\n",
    "            self.beta2t = 1\n",
    "            self.adam_initialized = True\n",
    "\n",
    "        alpha = learning_rate  # learning rate called alpha in original paper on adam\n",
    "        epsilon = 1e-8\n",
    "        error_trace = []\n",
    "        epochs_per_print = n_epochs // 10\n",
    "\n",
    "        for epoch in range(n_epochs):\n",
    "\n",
    "            error = error_f(*fargs)\n",
    "            grad = gradient_f(*fargs)\n",
    "\n",
    "            self.mt[:] = self.beta1 * self.mt + (1 - self.beta1) * grad\n",
    "            self.vt[:] = self.beta2 * self.vt + (1 - self.beta2) * grad * grad\n",
    "            self.beta1t *= self.beta1\n",
    "            self.beta2t *= self.beta2\n",
    "\n",
    "            m_hat = self.mt / (1 - self.beta1t)\n",
    "            v_hat = self.vt / (1 - self.beta2t)\n",
    "\n",
    "            # Update all weights using -= to modify their values in-place.\n",
    "            self.all_weights -= alpha * m_hat / (self.sqrt(v_hat) + epsilon)\n",
    "    \n",
    "            if error_convert_f:\n",
    "                error = error_convert_f(error)\n",
    "            error_trace.append(error)\n",
    "\n",
    "            if callback_f is not None:\n",
    "                callback_f(epoch)\n",
    "\n",
    "            if verbose and ((epoch + 1) % max(1, epochs_per_print) == 0):\n",
    "                print(f'Adam: Epoch {epoch+1:d} Error={error:.5f}')\n",
    "\n",
    "        return error_trace\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SCG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now the Scaled Conjugate Gradient, scg, algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile -a optimizers.py\n",
    "\n",
    "######################################################################\n",
    "#### scg\n",
    "######################################################################\n",
    "\n",
    "    def scg(self, error_f, gradient_f, fargs=[], n_epochs=100, error_convert_f=lambda x: x,\n",
    "            verbose=True, callback_f=None):\n",
    "\n",
    "        if not self.scg_initialized:\n",
    "            shape = self.all_weights.shape\n",
    "            self.w_new = np.zeros(shape)\n",
    "            self.w_temp = np.zeros(shape)\n",
    "            self.g_new = np.zeros(shape)\n",
    "            self.g_old = np.zeros(shape)\n",
    "            self.g_smallstep = np.zeros(shape)\n",
    "            self.search_dir = np.zeros(shape)\n",
    "            self.scg_initialized = True\n",
    "\n",
    "        sigma0 = 1.0e-6\n",
    "        fold = error_f(*fargs)\n",
    "        error = fold\n",
    "        self.g_new[:] = gradient_f(*fargs)\n",
    "        self.g_old[:] = copy.deepcopy(self.g_new)\n",
    "        self.search_dir[:] = -self.g_new\n",
    "        success = True\t\t\t\t# Force calculation of directional derivs.\n",
    "        nsuccess = 0\t\t\t\t# nsuccess counts number of successes.\n",
    "        beta = 1.0e-6\t\t\t\t# Initial scale parameter. Lambda in Moeller.\n",
    "        betamin = 1.0e-15 \t\t\t# Lower bound on scale.\n",
    "        betamax = 1.0e20\t\t\t# Upper bound on scale.\n",
    "        nvars = len(self.all_weights)\n",
    "        iteration = 1\t\t\t\t# j counts number of iterations\n",
    "        error_trace = []\n",
    "\n",
    "        error_trace.append(error_convert_f(error))\n",
    "\n",
    "        startTime = time.time()\n",
    "        startTimeLastVerbose = startTime\n",
    "\n",
    "        # Main optimization loop.\n",
    "        while iteration <= n_epochs:\n",
    "\n",
    "            # Calculate first and second directional derivatives.\n",
    "            if success:\n",
    "                mu = self.search_dir @ self.g_new\n",
    "                if mu >= 0:\n",
    "                    self.search_dir[:] = - self.g_new\n",
    "                    mu = self.search_dir.T @ self.g_new\n",
    "                kappa = self.search_dir.T @ self.search_dir\n",
    "                if math.isnan(kappa):\n",
    "                    print('kappa', kappa)\n",
    "\n",
    "                if kappa < sys.float_info.epsilon:\n",
    "                    return error_trace\n",
    "\n",
    "                sigma = sigma0 / math.sqrt(kappa)\n",
    "\n",
    "                self.w_temp[:] = self.all_weights\n",
    "                self.all_weights += sigma * self.search_dir\n",
    "                error_f(*fargs)  # forward pass through model for intermediate variable values for gradient\n",
    "                self.g_smallstep[:] = gradient_f(*fargs)\n",
    "                self.all_weights[:] = self.w_temp\n",
    "\n",
    "                theta = self.search_dir @ (self.g_smallstep - self.g_new) / sigma\n",
    "                if math.isnan(theta):\n",
    "                    print('theta', theta, 'sigma', sigma, 'search_dir[0]', self.search_dir[0], 'g_smallstep[0]', self.g_smallstep[0]) #, 'gradnew[0]', gradnew[0])\n",
    "\n",
    "            ## Increase effective curvature and evaluate step size alpha.\n",
    "\n",
    "            delta = theta + beta * kappa\n",
    "            # if math.isnan(scalarv(delta)):\n",
    "            if math.isnan(delta):\n",
    "                print('delta is NaN', 'theta', theta, 'beta', beta, 'kappa', kappa)\n",
    "            elif delta <= 0:\n",
    "                delta = beta * kappa\n",
    "                beta = beta - theta / kappa\n",
    "\n",
    "            if delta == 0:\n",
    "                success = False\n",
    "                fnow = fold\n",
    "            else:\n",
    "                alpha = -mu / delta\n",
    "                ## Calculate the comparison ratio Delta\n",
    "                self.w_temp[:] = self.all_weights\n",
    "                self.all_weights += alpha * self.search_dir\n",
    "                fnew = error_f(*fargs)\n",
    "                Delta = 2 * (fnew - fold) / (alpha * mu)\n",
    "                if not math.isnan(Delta) and Delta  >= 0:\n",
    "                    success = True\n",
    "                    nsuccess += 1\n",
    "                    # w[:] = wnew\n",
    "                    fnow = fnew\n",
    "\n",
    "                    if callback_f:\n",
    "                        callback_f(iteration)\n",
    "\n",
    "                else:\n",
    "                    success = False\n",
    "                    fnow = fold\n",
    "                    self.all_weights[:] = self.w_temp\n",
    "\n",
    "            iterationsPerPrint = math.ceil(n_epochs/10)\n",
    "            if verbose and iteration % max(1, iterationsPerPrint) == 0:\n",
    "                print('SCG: Iteration {:d} ObjectiveF={:.5f} Scale={:.3e} Seconds={:.3f}'.format(iteration,\n",
    "                                error_convert_f(fnow), beta, (time.time()-startTimeLastVerbose)))\n",
    "\n",
    "\n",
    "                startTimeLastVerbose = time.time()\n",
    "\n",
    "            # print('fnow', fnow, 'converted', error_convert_f(fnow))\n",
    "            error_trace.append(error_convert_f(fnow))\n",
    "\n",
    "            if success:\n",
    "\n",
    "                fold = fnew\n",
    "                self.g_old[:] = self.g_new\n",
    "                self.g_new[:] = gradient_f(*fargs)\n",
    "\n",
    "                # If the gradient is zero then we are done.\n",
    "                gg = self.g_new @ self.g_new  # dot(gradnew, gradnew)\n",
    "                if gg == 0:\n",
    "                    return error_trace\n",
    "\n",
    "            if math.isnan(Delta) or Delta < 0.25:\n",
    "                beta = min(4.0 * beta, betamax)\n",
    "            elif Delta > 0.75:\n",
    "                beta = max(0.5 * beta, betamin)\n",
    "\n",
    "            # Update search direction using Polak-Ribiere formula, or re-start\n",
    "            # in direction of negative gradient after nparams steps.\n",
    "            if nsuccess == nvars:\n",
    "                self.search_dir[:] = -self.g_new\n",
    "                nsuccess = 0\n",
    "            elif success:\n",
    "                gamma = (self.g_old - self.g_new) @ (self.g_new / mu)\n",
    "                #self.search_dir[:] = gamma * self.search_dir - self.g_new\n",
    "                self.search_dir *= gamma\n",
    "                self.search_dir -= self.g_new\n",
    "\n",
    "            iteration += 1\n",
    "\n",
    "            # If we get here, then we haven't terminated in the given number of\n",
    "            # iterations.\n",
    "\n",
    "        return error_trace\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demonstrations on Simple Optimization Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each is illustrated here as they search for the minimum of the following 3-dimensional bowl."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-29T18:50:25.111884Z",
     "start_time": "2021-07-29T18:50:25.101231Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import IPython.display as ipd  # for display and clear_output\n",
    "import time  # for sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-29T18:50:26.596483Z",
     "start_time": "2021-07-29T18:50:26.586918Z"
    }
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.colors import LightSource\n",
    "\n",
    "# x will be set at top level, so a global variable\n",
    "# x will be row vector\n",
    "\n",
    "def parabola(xmin, s):\n",
    "    d = x.reshape(-1, 1) - xmin\n",
    "    return d.T @ s @ d\n",
    "\n",
    "def parabola_gradient(xmin, s):\n",
    "    d = x.reshape(-1, 1) - xmin\n",
    "    return 2 * (s @ d).reshape(-1)  # must be row vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-29T18:50:32.126652Z",
     "start_time": "2021-07-29T18:50:31.992267Z"
    }
   },
   "outputs": [],
   "source": [
    "!head -20 optimizers.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "center = np.array([5, 5]).reshape(2, 1)\n",
    "S = np.array([[5, 3], [3, 5]])\n",
    "\n",
    "n = 20\n",
    "xs = np.linspace(0, 10, n)\n",
    "ys = np.linspace(0, 10, n)\n",
    "X,Y = np.meshgrid(xs, ys)\n",
    "both = np.vstack((X.flat, Y.flat)).T\n",
    "nall = n * n\n",
    "\n",
    "Z = np.zeros(nall)\n",
    "for i in range(nall):\n",
    "    x = both[i:i + 1, :]\n",
    "    Z[i] = parabola(center, S)\n",
    "Z = Z.reshape(n, n)\n",
    "\n",
    "# see https://matplotlib.org/3.1.0/gallery/mplot3d/surface3d.html\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "ax = plt.axes(projection='3d')\n",
    "\n",
    "# Plot the surface.\n",
    "surf = ax.plot_surface(X, Y, Z, linewidth=0, antialiased=False,\n",
    "                       cmap=plt.cm.coolwarm)\n",
    "ax.view_init(elev=30., azim=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-29T18:50:33.913546Z",
     "start_time": "2021-07-29T18:50:33.875160Z"
    }
   },
   "outputs": [],
   "source": [
    "import optimizers as opt\n",
    "\n",
    "def show_trace(fig, x_start, function, function_gradient, function_args, n_epochs, learning_rates):\n",
    "    global x\n",
    "    \n",
    "    x = x_start.copy()\n",
    "    sgd_weights_trace = [x.copy()]\n",
    "    def callback(epoch):\n",
    "        sgd_weights_trace.append(x.copy())\n",
    "    optimizer = opt.Optimizers(x)\n",
    "    errors_sgd = optimizer.sgd(function, function_gradient, function_args,\n",
    "                               n_epochs=n_epochs, learning_rate=learning_rates[0],\n",
    "                               callback_f=callback, verbose=False)\n",
    "    \n",
    "    x = x_start.copy()\n",
    "    adam_weights_trace = [x.copy()]\n",
    "    def callback(epoch):\n",
    "        adam_weights_trace.append(x.copy())\n",
    "    optimizer = opt.Optimizers(x)\n",
    "    errors_adam = optimizer.adam(function, function_gradient, function_args,\n",
    "                                 n_epochs=n_epochs, learning_rate=learning_rates[1],\n",
    "                                 callback_f=callback, verbose=False)\n",
    "    \n",
    "    x = x_start.copy()\n",
    "    scg_weights_trace = [x.copy()]\n",
    "    def callback(epoch):\n",
    "        scg_weights_trace.append(x.copy())\n",
    "    optimizer = opt.Optimizers(x)\n",
    "    errors_scg = optimizer.scg(function, function_gradient, function_args,\n",
    "                               n_epochs=200,\n",
    "                               callback_f=callback, verbose=False)\n",
    "\n",
    "    plt.clf()\n",
    "    \n",
    "    xt = np.array(sgd_weights_trace)\n",
    "    plt.plot(xt[:, 0], xt[:, 1], 'ro-', alpha=0.4, label='SGD')\n",
    "\n",
    "    xt = np.array(adam_weights_trace)\n",
    "    plt.plot(xt[:, 0], xt[:, 1], 'go-', alpha=0.2, label='Adam')\n",
    "\n",
    "    xt = np.array(scg_weights_trace)\n",
    "    plt.plot(xt[:, 0], xt[:, 1], 'ko-', alpha=0.4, label='SCG')\n",
    "\n",
    "    plt.contourf(X, Y, Z, 20, alpha=0.3)\n",
    "    plt.axis('tight')\n",
    "    \n",
    "    plt.legend()\n",
    "    ipd.clear_output(wait=True)\n",
    "    ipd.display(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-29T18:50:37.697170Z",
     "start_time": "2021-07-29T18:50:37.438072Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 10))\n",
    "\n",
    "x_start = np.random.uniform(0, 10, 2)\n",
    "show_trace(fig, x_start, parabola, parabola_gradient, [center, S], \n",
    "           n_epochs=200, learning_rates=[0.01, 0.5])\n",
    "\n",
    "ipd.clear_output(wait=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rosenbrock's function is often used to test optimization algorithms.\n",
    "It is\n",
    "\n",
    "$$\n",
    "f(x,y) = (1-x)^2 + 100(y-x^2)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rosen():\n",
    "    v = (1.0 - x[0])**2 + 100 * ((x[1] - x[0]**2)**2)\n",
    "    return v\n",
    "\n",
    "def rosen_gradient():\n",
    "    g1 = -400 * (x[1] - x[0]**2) * x[0] - 2 * (1 - x[0])\n",
    "    g2 =  200 * (x[1] - x[0]**2)\n",
    "    return np.array([g1, g2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10\n",
    "xmin, xmax = -1,2\n",
    "xs = np.linspace(xmin, xmax, n)\n",
    "ys = np.linspace(xmin, xmax, n)\n",
    "X, Y = np.meshgrid(xs, ys)    \n",
    "    \n",
    "both = np.vstack((X.flat, Y.flat)).T\n",
    "nall = n * n\n",
    "Z = np.zeros(nall)\n",
    "for i in range(n * n):\n",
    "    x = both[i]\n",
    "    Z[i] = rosen()\n",
    "Z.resize((n, n))\n",
    "\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "ax = plt.axes(projection='3d')\n",
    "\n",
    "# Plot the surface.\n",
    "surf = ax.plot_surface(X, Y, Z, linewidth=0, antialiased=False,\n",
    "                       cmap=plt.cm.coolwarm)\n",
    "ax.view_init(elev=40., azim=260)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 10))\n",
    "\n",
    "x_start = np.random.uniform(-1, 2, 2)\n",
    "show_trace(fig, x_start, rosen, rosen_gradient, [], n_epochs=400, \n",
    "           learning_rates=[0.0001, 0.5])\n",
    "    \n",
    "ipd.clear_output(wait=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
