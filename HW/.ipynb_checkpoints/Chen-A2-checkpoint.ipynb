{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A2: NeuralNetwork Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Requirements\" data-toc-modified-id=\"Requirements-1\">Requirements</a></span></li><li><span><a href=\"#Code-for-NeuralNetwork-Class\" data-toc-modified-id=\"Code-for-NeuralNetwork-Class-2\">Code for <code>NeuralNetwork</code> Class</a></span></li><li><span><a href=\"#Example-Results\" data-toc-modified-id=\"Example-Results-3\">Example Results</a></span></li><li><span><a href=\"#Application-to-Boston-Housing-Data\" data-toc-modified-id=\"Application-to-Boston-Housing-Data-4\">Application to Boston Housing Data</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, you will complete the implementation of the `NeuralNetwork` class, starting with the code included in the next code cell.  Your implementation must meet the requirements described in the doc-strings.\n",
    "\n",
    "Download [optimizers.tar](https://www.cs.colostate.edu/~anderson/cs545/notebooks/optimizers.tar) and extract `optimizers.py` for use in this assignment.\n",
    "\n",
    "Then apply your `NeuralNetwork` class to the problem of predicting the value of houses in Boston as described below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code for `NeuralNetwork` Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting neuralnetwork.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile neuralnetwork.py\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "import time  # for sleep\n",
    "import IPython.display as ipd  # for display and clear_output\n",
    "from IPython.display import display, clear_output  # for the following animation\n",
    "import os\n",
    "import copy\n",
    "import signal\n",
    "import os\n",
    "import numpy as np\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.colors import LightSource\n",
    "import optimizers as opt\n",
    "\n",
    "\n",
    "class NeuralNetwork():\n",
    "    \"\"\"\n",
    "    A class that represents a neural network for nonlinear regression\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    n_inputs : int\n",
    "        The number of values in each sample\n",
    "    n_hidden_units_by_layers: list of ints, or empty\n",
    "        The number of units in each hidden layer.\n",
    "        Its length specifies the number of hidden layers.\n",
    "    n_outputs: int\n",
    "        The number of units in output layer\n",
    "    all_weights : one-dimensional numpy array\n",
    "        Contains all weights of the network as a vector\n",
    "    Ws : list of two-dimensional numpy arrays\n",
    "        Contains matrices of weights in each layer,\n",
    "        as views into all_weights\n",
    "    all_gradients : one-dimensional numpy array\n",
    "        Contains all gradients of mean square error with\n",
    "        respect to each weight in the network as a vector\n",
    "    Grads : list of two-dimensional numpy arrays\n",
    "        Contains matrices of gradients weights in each layer,\n",
    "        as views into all_gradients\n",
    "    total_epochs : int\n",
    "        Total number of epochs trained so far\n",
    "    error_trace : list\n",
    "        Mean square error (standardized) after each epoch\n",
    "    X_means : one-dimensional numpy array\n",
    "        Means of the components, or features, across samples\n",
    "    X_stds : one-dimensional numpy array\n",
    "        Standard deviations of the components, or features, across samples\n",
    "    T_means : one-dimensional numpy array\n",
    "        Means of the components of the targets, across samples\n",
    "    T_stds : one-dimensional numpy array\n",
    "        Standard deviations of the components of the targets, across samples\n",
    "        \n",
    "    Methods\n",
    "    -------\n",
    "    make_weights_and_views(shapes)\n",
    "        Creates all initial weights and views for each layer\n",
    "\n",
    "    train(X, T, n_epochs, method='sgd', learning_rate=None, verbose=True)\n",
    "        Trains the network using samples by rows in X and T\n",
    "\n",
    "    use(X)\n",
    "        Applies network to inputs X and returns network's output\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_inputs, n_hidden_units_by_layers, n_outputs):\n",
    "        \"\"\"Creates a neural network with the given structure\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        n_inputs : int\n",
    "            The number of values in each sample\n",
    "        n_hidden_units_by_layers : list of ints, or empty\n",
    "            The number of units in each hidden layer.\n",
    "            Its length specifies the number of hidden layers.\n",
    "        n_outputs : int\n",
    "            The number of units in output layer\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        NeuralNetwork object\n",
    "        \"\"\"\n",
    "        \n",
    "        self.n_inputs = n_inputs\n",
    "        self.n_inputs = n_inputs =  n_inputs = n_inputs\n",
    "        self.n_outputs = n_outputs\n",
    "        self.n_hidden_units_by_layers = n_hidden_units_by_layers\n",
    "        self.Ws= []\n",
    "        self.all_weights=np.empty([0,1])\n",
    "        self.shapes=[]\n",
    " \n",
    "    \n",
    "        layer_n=n_inputs\n",
    "        for layerI in range(len(self.n_hidden_units_by_layers)):\n",
    "            layerI_N=self.n_hidden_units_by_layers[layerI]\n",
    "            self.shapes.append([1 + layer_n, layerI_N])\n",
    "            layer_n=layerI_N\n",
    "        self.shapes.append([1 + layer_n, n_outputs])\n",
    "        \n",
    "        self.make_weights_and_views(self.shapes)\n",
    "        self.all_gradients = []\n",
    "        self.Grads = []\n",
    "        self.total_epochs = 0\n",
    "        self.error_trace = []\n",
    "        self.X_means = None\n",
    "        self.X_stds = None\n",
    "        self.T_means = None\n",
    "        self.T_stds = None\n",
    "        self.Ys=None\n",
    "        \n",
    "        # Assign attribute values. Set self.X_means to None to indicate\n",
    "        # that standardization parameters have not been calculated.\n",
    "        # ....\n",
    "  \n",
    "\n",
    "        # Build list of shapes for weight matrices in each layer\n",
    "        # ...\n",
    "        \n",
    "        # Call make_weights_and_views to create all_weights and Ws\n",
    "        # ...\n",
    "        \n",
    "        # Call make_weights_and_views to create all_gradients and Grads\n",
    "        # ...\n",
    "\n",
    "    def make_weights_and_views(self, shapes):\n",
    "        \"\"\"Creates vector of all weights and views for each layer\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        shapes : list of pairs of ints\n",
    "            Each pair is number of rows and columns of weights in each layer\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Vector of all weights, and list of views into this vector for each layer\n",
    "        \"\"\"\n",
    "\n",
    "        for layerI in range(len(shapes)):\n",
    "            shapeX=shapes[layerI][0]\n",
    "            shapeY=shapes[layerI][1]\n",
    "            wI=1 / np.sqrt(shapeX) * np.random.uniform(-1, 1, size=(shapeX, shapeY))\n",
    "            self.Ws.append(wI)\n",
    "#                       haha=np.vstack((self.all_weights,wI.reshape(-1,1)))\n",
    "            self.all_weights=np.vstack((self.all_weights,wI.reshape(-1,1))) \n",
    " \n",
    "\n",
    "        # Create one-dimensional numpy array of all weights with random initial values\n",
    "        #  ...\n",
    "\n",
    "        # Build list of views by reshaping corresponding elements\n",
    "        # from vector of all weights into correct shape for each layer.        \n",
    "        # ...\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'NeuralNetwork({self.n_inputs}, ' + \\\n",
    "            f'{self.n_hidden_units_by_layers}, {self.n_outputs})'\n",
    "\n",
    "    def __str__(self):\n",
    "        s = self.__repr__()\n",
    "        if self.total_epochs > 0:\n",
    "            s += f'\\n Trained for {self.total_epochs} epochs.'\n",
    "            s += f'\\n Final standardized training error {self.error_trace[-1]:.4g}.'\n",
    "        return s\n",
    " \n",
    "    def train(self, X, T, n_epochs, method='sgd', learning_rate=None, verbose=True):\n",
    "        \"\"\"Updates the weights \n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : two-dimensional numpy array\n",
    "            number of samples  x  number of input components\n",
    "        T : two-dimensional numpy array\n",
    "            number of samples  x  number of output components\n",
    "        n_epochs : int\n",
    "            Number of passes to take through all samples\n",
    "        method : str\n",
    "            'sgd', 'adam', or 'scg'\n",
    "        learning_rate : float\n",
    "            Controls the step size of each update, only for sgd and adam\n",
    "        verbose: boolean\n",
    "            If True, progress is shown with print statements\n",
    "        \"\"\"\n",
    "\n",
    "        # Calculate and assign standardization parameters\n",
    "        # ...\n",
    "\n",
    "        # Standardize X and T\n",
    "        # ...\n",
    "\n",
    "        # Instantiate Optimizers object by giving it vector of all weights\n",
    "        self.X_means = np.mean(X, axis=0)\n",
    "        self.X_stds = np.std(X, axis=0)\n",
    "        self.T_means = np.mean(T, axis=0)\n",
    "        self.T_stds = np.std(T, axis=0)\n",
    "        # Standardize X and T\n",
    "\n",
    "        X = (X - self.X_means) / self.X_stds\n",
    "        T = (T - self.T_means) / self.T_stds\n",
    " \n",
    "        # Instantiate Optimizers object by giving it vector of all weights\n",
    "        optimizer = opt.Optimizers(self.all_weights)\n",
    "\n",
    "        error_convert_f = lambda err: (np.sqrt(err) * self.T_stds)[0]\n",
    "        \n",
    "        # Call the requested optimizer method to train the weights.\n",
    "\n",
    "        if method == 'sgd':\n",
    "            error_trace=optimizer.sgd(self.error_f, self.gradient_f,fargs=[X,T],error_convert_f=error_convert_f,learning_rate=learning_rate,n_epochs=n_epochs,verbose=True)\n",
    "        elif method == 'adam':\n",
    "            error_trace=optimizer.adam(self.error_f, self.gradient_f,fargs=[X,T],error_convert_f=error_convert_f,learning_rate=learning_rate,n_epochs=n_epochs)\n",
    "        elif method == 'scg':\n",
    "            error_trace=optimizer.scg(self.error_f, self.gradient_f,fargs=[X,T],error_convert_f=error_convert_f,n_epochs=n_epochs)\n",
    "        else:\n",
    "            raise Exception(\"method must be 'sgd', 'adam', or 'scg'\")\n",
    " \n",
    "    \n",
    "        self.total_epochs += len(error_trace)\n",
    "        self.error_trace += error_trace\n",
    "\n",
    "\n",
    "\n",
    "        self._forward(X)\n",
    "        error = (T - self.Ys[-1]) * self.T_stds \n",
    "  \n",
    "        # Return neural network object to allow applying other methods\n",
    "        # after training, such as:    Y = nnet.train(X, T, 100, 0.01).use(X)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def addOnes(self,A):\n",
    "        return np.insert(A, 0, 1, axis=1)\n",
    "    \n",
    "    def _forward(self, X):\n",
    "        \"\"\"Calculate outputs of each layer given inputs in X\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : input samples, standardized\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Outputs of all layers as list\n",
    "        \"\"\"\n",
    "        i=0\n",
    "        for layerI in range(len(self.shapes)):\n",
    "            shapeX=self.shapes[layerI][0]\n",
    "            shapeY=self.shapes[layerI][1]\n",
    "            self.Ws[layerI]=self.all_weights[i:i+shapeX*shapeY].reshape(shapeX,shapeY)\n",
    "            i+=shapeX*shapeY\n",
    "        \n",
    "        self.Ys=[]\n",
    "        for layerI in range(len(self.n_hidden_units_by_layers)):\n",
    "            X=np.tanh(self.addOnes(X) @ self.Ws[layerI])\n",
    "            self.Ys.append(X)\n",
    "        X=self.addOnes(X)@self.Ws[-1]\n",
    "        self.Ys.append(X)\n",
    "        # Append output of each layer to list in self.Ys, then return it.\n",
    "        # ...\n",
    "\n",
    "    # Function to be minimized by optimizer method, mean squared error\n",
    "    def error_f(self, X, T):\n",
    "        \"\"\"Calculate output of net and its mean squared error \n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : two-dimensional numpy array\n",
    "            number of samples  x  number of input components\n",
    "        T : two-dimensional numpy array\n",
    "            number of samples  x  number of output components\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Mean square error as scalar float that is the mean\n",
    "        square error over all samples\n",
    "        \"\"\"\n",
    "        self._forward(X)\n",
    "        error = (T - self.Ys[-1]) * self.T_stds \n",
    "        self.error_trace.append(error)\n",
    "        summation = 0  #variable to store the summation of differences\n",
    "        n = len(error) #finding total number of items in list\n",
    "        for i in range (n):  #looping through each element of the list\n",
    "            difference = error[i]**2\n",
    "            summation +=difference  \n",
    "        MSE = summation/n  #dividing summation by total values to obtain average\n",
    " \n",
    "        return MSE\n",
    "    \n",
    "        # Call _forward, calculate mean square error and return it.\n",
    "        # ...\n",
    "\n",
    "    # Gradient of function to be minimized for use by optimizer method\n",
    "    def gradient_f(self, X, T):\n",
    "        \"\"\"Returns gradient wrt all weights. Assumes _forward already called.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : two-dimensional numpy array\n",
    "            number of samples  x  number of input components\n",
    "        T : two-dimensional numpy array\n",
    "            number of samples  x  number of output components\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Vector of gradients of mean square error wrt all weights\n",
    "        \"\"\"\n",
    "\n",
    "        # Assumes forward_pass just called with layer outputs saved in self.Ys.\n",
    "        self._forward( X)\n",
    "        # Assumes forward_pass just called with layer outputs saved in self.Ys.\n",
    "        n_samples = X.shape[0]\n",
    "        n_outputs = T.shape[1]\n",
    "        n_layers = len(self.n_hidden_units_by_layers) + 1\n",
    "\n",
    "        # D is delta matrix to be back propagated\n",
    "        D = -(T - self.Ys[-1]) / (n_samples * n_outputs)\n",
    "        self.Grads =  [None] *n_layers\n",
    "\n",
    "        # Step backwards through the layers to back-propagate the error (D)\n",
    "        for layeri in range(n_layers - 1, -1, -1):\n",
    "            # gradient of all but bias weights\n",
    " \n",
    "            # Back-propagate this layer's delta to previous layer\n",
    "            if layeri > 0:\n",
    "                self.Grads[layeri]= self.addOnes(self.Ys[layeri-1]).T@D\n",
    "                D =D@self.Ws[layeri][1:,:].T*(1-self.Ys[layeri-1]**2)  \n",
    "            else:\n",
    "                self.Grads[layeri]= -self.addOnes(X).T@D\n",
    "         \n",
    "        \n",
    "        self.all_gradients=np.empty([0,1])\n",
    "        \n",
    "        for layerI in range(n_layers):          \n",
    "            self.all_gradients=np.vstack((self.all_gradients,self.Grads[layerI].reshape(-1,1))) \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "        return self.all_gradients\n",
    "\n",
    "    def use(self, X):\n",
    "        \"\"\"Return the output of the network for input samples as rows in X\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : two-dimensional numpy array\n",
    "            number of samples  x  number of input components, unstandardized\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Output of neural network, unstandardized, as numpy array\n",
    "        of shape  number of samples  x  number of outputs\n",
    "        \"\"\"\n",
    "        X=(X-self.X_means)/self.X_stds\n",
    "        self._forward( X)\n",
    "        Y=self.Ys[-1]\n",
    "        Y=Y*self.T_stds+self.T_means\n",
    "        return Y \n",
    "        # Standardize X\n",
    "        # ...\n",
    "        \n",
    "        # Unstandardize output Y before returning it\n",
    "        return ...\n",
    "\n",
    "    def get_error_trace(self):\n",
    "        \"\"\"Returns list of standardized mean square error for each epoch\"\"\"\n",
    "        return self.error_trace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we test the `NeuralNetwork` class with some simple data.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sgd: Epoch 5000 ObjectiveF=0.28906\n",
      "sgd: Epoch 10000 ObjectiveF=0.28889\n",
      "sgd: Epoch 15000 ObjectiveF=0.28924\n",
      "sgd: Epoch 20000 ObjectiveF=0.28917\n",
      "sgd: Epoch 25000 ObjectiveF=0.28879\n",
      "sgd: Epoch 30000 ObjectiveF=0.28906\n",
      "sgd: Epoch 35000 ObjectiveF=0.28893\n",
      "sgd: Epoch 40000 ObjectiveF=0.28879\n",
      "sgd: Epoch 45000 ObjectiveF=0.28888\n",
      "sgd: Epoch 50000 ObjectiveF=0.28895\n",
      "Adam: Epoch 5000 ObjectiveF=0.28043\n",
      "Adam: Epoch 10000 ObjectiveF=0.28638\n",
      "Adam: Epoch 15000 ObjectiveF=0.28772\n",
      "Adam: Epoch 20000 ObjectiveF=0.28648\n",
      "Adam: Epoch 25000 ObjectiveF=0.28897\n",
      "Adam: Epoch 30000 ObjectiveF=0.28873\n",
      "Adam: Epoch 35000 ObjectiveF=0.28839\n",
      "Adam: Epoch 40000 ObjectiveF=0.28780\n",
      "Adam: Epoch 45000 ObjectiveF=0.28821\n",
      "Adam: Epoch 50000 ObjectiveF=0.28815\n",
      "SCG: Epoch 5000 ObjectiveF=0.28879\n",
      "SCG: Epoch 10000 ObjectiveF=0.28879\n",
      "SCG: Epoch 15000 ObjectiveF=0.28879\n",
      "SCG: Epoch 20000 ObjectiveF=0.28879\n",
      "SCG: Epoch 25000 ObjectiveF=0.28879\n",
      "SCG: Epoch 30000 ObjectiveF=0.28879\n",
      "SCG: Epoch 35000 ObjectiveF=0.28879\n",
      "SCG: Epoch 40000 ObjectiveF=0.28879\n",
      "SCG: Epoch 45000 ObjectiveF=0.28879\n",
      "SCG: Epoch 50000 ObjectiveF=0.28879\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import neuralnetwork as nn\n",
    "\n",
    "X = np.arange(-2, 2, 0.05).reshape(-1, 1)\n",
    "T = np.sin(X) * np.sin(X * 10)\n",
    "\n",
    "errors = []\n",
    "n_epochs = 1000\n",
    "method_rhos = [('sgd', 0.01),\n",
    "               ('adam', 0.005),\n",
    "               ('scg', None)]\n",
    "\n",
    "for method, rho in method_rhos:\n",
    "    nnet = nn.NeuralNetwork(X.shape[1], [10, 10], 1)\n",
    "    nnet.train(X, T, 50000, method=method, learning_rate=rho)\n",
    "    Y = nnet.use(X)\n",
    "    plt.plot(X, Y, 'o-', label='Model ' + method)\n",
    "    errors.append(nnet.get_error_trace())\n",
    "\n",
    "plt.plot(X, T, 'o', label='Train')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('T or Y')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chen/.local/lib/python3.8/site-packages/numpy/core/shape_base.py:65: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  ary = asanyarray(ary)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;31mTypeError\u001b[0m: only size-1 arrays can be converted to Python scalars",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-527d1c346767>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0merror_trace\u001b[0m \u001b[0;32min\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_trace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Epoch'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Standardized error'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2785\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mdocstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2786\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscalex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaley\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2787\u001b[0;31m     return gca().plot(\n\u001b[0m\u001b[1;32m   2788\u001b[0m         *args, scalex=scalex, scaley=scaley, **({\"data\": data} if data\n\u001b[1;32m   2789\u001b[0m         is not None else {}), **kwargs)\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1665\u001b[0m         \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1666\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1667\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1668\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautoscale_view\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscalex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscalex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaley\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscaley\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1669\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36madd_line\u001b[0;34m(self, line)\u001b[0m\n\u001b[1;32m   1900\u001b[0m             \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_clip_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1902\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_line_limits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1903\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1904\u001b[0m             \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_line%d'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_update_line_limits\u001b[0;34m(self, line)\u001b[0m\n\u001b[1;32m   1922\u001b[0m         \u001b[0mFigures\u001b[0m \u001b[0mout\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdata\u001b[0m \u001b[0mlimit\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mgiven\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdating\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataLim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1923\u001b[0m         \"\"\"\n\u001b[0;32m-> 1924\u001b[0;31m         \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1925\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvertices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1926\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/matplotlib/lines.py\u001b[0m in \u001b[0;36mget_path\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1025\u001b[0m         \"\"\"\n\u001b[1;32m   1026\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_invalidy\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_invalidx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1027\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1028\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1029\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/matplotlib/lines.py\u001b[0m in \u001b[0;36mrecache\u001b[0;34m(self, always)\u001b[0m\n\u001b[1;32m    673\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0malways\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_invalidy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m             \u001b[0myconv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_yunits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_yorig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 675\u001b[0;31m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_to_unmasked_float_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myconv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    676\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_y\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/matplotlib/cbook/__init__.py\u001b[0m in \u001b[0;36m_to_unmasked_float_array\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   1388\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1389\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1390\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(2)\n",
    "plt.clf()\n",
    "for error_trace in errors:\n",
    "    plt.plot(error_trace)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Standardized error')\n",
    "plt.legend([mr[0] for mr in method_rhos]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your results will not be the same, but your code should complete and make plots somewhat similar to these."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application to Boston Housing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download data from [Boston House Data at Kaggle](https://www.kaggle.com/fedesoriano/the-boston-houseprice-data). Read it into python using the `pandas.read_csv` function.  Assign the first 13 columns as inputs to `X` and the final column as target values to `T`.  Make sure `T` is two-dimensional."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before training your neural networks, partition the data into training and testing partitions, as shown here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition(X, T, train_fraction):\n",
    "    n_samples = X.shape[0]\n",
    "    rows = np.arange(n_samples)\n",
    "    np.random.shuffle(rows)\n",
    "    \n",
    "    n_train = round(n_amples * train_fraction)\n",
    "    \n",
    "    Xtrain = X[rows[:ntrain], :]\n",
    "    Ttrain = T[rows[:ntrain], :]\n",
    "    Xtest = X[rows[ntrain:], :]\n",
    "    Ttest = T[rows[ntrain:], :]\n",
    "    \n",
    "def rmse(T, Y):\n",
    "    return np.sqrt(np.mean((T - Y)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have assigned `X` and `T` correctly.\n",
    "\n",
    "Xtrain, Train, Xtest, Ttest = partition(X, T, 0.8)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write and run code using your `NeuralNetwork` class to model the Boston housing data. Experiment with all three optimization methods and a variety of neural network structures (numbers of hidden layer and units), learning rates, and numbers of epochs. Show results for at least three different network structures, learning rates, and numbers of epochs for each method.  Show your results using print statements that include the method, network structure, number of epochs, learning rate, and RMSE on training data and RMSE on testing data.\n",
    "\n",
    "Try to find good values for the RMSE on testing data.  Discuss your results, including how good you think the RMSE values are by considering the range of house values given in the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grading\n",
    "\n",
    "<font color='red'>A2grader.tar is coming soon.</font>\n",
    "\n",
    "Your notebook will be run and graded automatically. Test this grading process by first downloading [A2grader.tar](http://www.cs.colostate.edu/~anderson/cs545/notebooks/A2grader.tar) and extract `A2grader.py` from it. Run the code in the following cell to demonstrate an example grading session.  The remaining 20 points will be based on your discussion of this assignment.\n",
    "\n",
    "A different, but similar, grading script will be used to grade your checked-in notebook. It will include additional tests. You should design and perform additional tests on all of your functions to be sure they run correctly before checking in your notebook.  \n",
    "\n",
    "For the grading script to run correctly, you must first name this notebook as 'Lastname-A2.ipynb' with 'Lastname' being your last name, and then save this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================= Code Execution =======================\n",
      "\n",
      "Extracting python code from notebook named 'Chen-A2.ipynb' and storing in notebookcode.py\n",
      "Removing all statements that are not function or class defs or import statements.\n",
      "\n",
      "Testing\n",
      "\n",
      "    import neuralnetwork as nn\n",
      "\n",
      "    n_inputs = 3\n",
      "    n_hiddens = [10, 20]\n",
      "    n_outputs = 2\n",
      "    n_samples = 5\n",
      "\n",
      "    X = np.arange(n_samples * n_inputs).reshape(n_samples, n_inputs) * 0.1\n",
      "    \n",
      "    nnet = nn.NeuralNetwork(n_inputs, n_hiddens, n_outputs)\n",
      "    nnet.all_weights = 0.1  # set all weights to 0.1\n",
      "    nnet.X_means = np.mean(X, axis=0)\n",
      "    nnet.X_stds = np.std(X, axis=0)\n",
      "    nnet.T_means = np.zeros((n_samples, n_outputs))\n",
      "    nnet.T_stds = np.ones((n_samples, n_outputs))\n",
      "    \n",
      "    Y = nnet.use(X)\n",
      "\n",
      "\n",
      "--- 40/40 points. Returned correct value.\n",
      "\n",
      "Testing\n",
      "    n_inputs = 3\n",
      "    n_hiddens = [10, 500, 6, 3]\n",
      "    n_samples = 5\n",
      "\n",
      "    X = np.arange(n_samples * n_inputs).reshape(n_samples, n_inputs) * 0.1\n",
      "    T = np.log(X + 0.1)\n",
      "    n_outputs = T.shape[1]\n",
      "    \n",
      "    def rmse(A, B):\n",
      "        return np.sqrt(np.mean((A - B)**2))\n",
      "\n",
      "    results = []\n",
      "    for rep in range(20):\n",
      "        nnet = nn.NeuralNetwork(n_inputs, n_hiddens, n_outputs)\n",
      "        nnet.train(X, T, 5000, 'adam', 0.001, verbose=False)\n",
      "        Y = nnet.use(X)\n",
      "        err = rmse(Y, T)\n",
      "        print(f'Net {rep+1} RMSE {err:.5f}')\n",
      "        results.append(err)\n",
      "\n",
      "    mean_rmse = np.mean(results)\n",
      "    print(mean_rmse)\n",
      "\n",
      "Adam: Epoch 500 ObjectiveF=0.03940\n",
      "Adam: Epoch 1000 ObjectiveF=0.02396\n",
      "Adam: Epoch 1500 ObjectiveF=0.01582\n",
      "Adam: Epoch 2000 ObjectiveF=0.01036\n",
      "Adam: Epoch 2500 ObjectiveF=0.00608\n",
      "Adam: Epoch 3000 ObjectiveF=0.00325\n",
      "Adam: Epoch 3500 ObjectiveF=0.00160\n",
      "Adam: Epoch 4000 ObjectiveF=0.00089\n",
      "Adam: Epoch 4500 ObjectiveF=0.00070\n",
      "Adam: Epoch 5000 ObjectiveF=0.00067\n",
      "Net 1 RMSE 0.00058\n",
      "Adam: Epoch 500 ObjectiveF=0.16897\n",
      "Adam: Epoch 1000 ObjectiveF=0.07874\n",
      "Adam: Epoch 1500 ObjectiveF=0.03738\n",
      "Adam: Epoch 2000 ObjectiveF=0.02052\n",
      "Adam: Epoch 2500 ObjectiveF=0.01397\n",
      "Adam: Epoch 3000 ObjectiveF=0.01075\n",
      "Adam: Epoch 3500 ObjectiveF=0.00876\n",
      "Adam: Epoch 4000 ObjectiveF=0.00711\n",
      "Adam: Epoch 4500 ObjectiveF=0.00632\n",
      "Adam: Epoch 5000 ObjectiveF=0.00539\n",
      "Net 2 RMSE 0.00396\n",
      "Adam: Epoch 500 ObjectiveF=0.24755\n",
      "Adam: Epoch 1000 ObjectiveF=0.03637\n",
      "Adam: Epoch 1500 ObjectiveF=0.02042\n",
      "Adam: Epoch 2000 ObjectiveF=0.00737\n",
      "Adam: Epoch 2500 ObjectiveF=0.00378\n",
      "Adam: Epoch 3000 ObjectiveF=0.00211\n",
      "Adam: Epoch 3500 ObjectiveF=0.00137\n",
      "Adam: Epoch 4000 ObjectiveF=0.00088\n",
      "Adam: Epoch 4500 ObjectiveF=0.00052\n",
      "Adam: Epoch 5000 ObjectiveF=0.00046\n",
      "Net 3 RMSE 0.00119\n",
      "Adam: Epoch 500 ObjectiveF=0.19274\n",
      "Adam: Epoch 1000 ObjectiveF=0.02412\n",
      "Adam: Epoch 1500 ObjectiveF=0.01896\n",
      "Adam: Epoch 2000 ObjectiveF=0.01700\n",
      "Adam: Epoch 2500 ObjectiveF=0.01475\n",
      "Adam: Epoch 3000 ObjectiveF=0.01208\n",
      "Adam: Epoch 3500 ObjectiveF=0.00904\n",
      "Adam: Epoch 4000 ObjectiveF=0.00592\n",
      "Adam: Epoch 4500 ObjectiveF=0.00578\n",
      "Adam: Epoch 5000 ObjectiveF=0.00220\n",
      "Net 4 RMSE 0.00163\n",
      "Adam: Epoch 500 ObjectiveF=0.22007\n",
      "Adam: Epoch 1000 ObjectiveF=0.04101\n",
      "Adam: Epoch 1500 ObjectiveF=0.01170\n",
      "Adam: Epoch 2000 ObjectiveF=0.00249\n",
      "Adam: Epoch 2500 ObjectiveF=0.00034\n",
      "Adam: Epoch 3000 ObjectiveF=0.00024\n",
      "Adam: Epoch 3500 ObjectiveF=0.00026\n",
      "Adam: Epoch 4000 ObjectiveF=0.00027\n",
      "Adam: Epoch 4500 ObjectiveF=0.00091\n",
      "Adam: Epoch 5000 ObjectiveF=0.00645\n",
      "Net 5 RMSE 0.00278\n",
      "Adam: Epoch 500 ObjectiveF=0.10857\n",
      "Adam: Epoch 1000 ObjectiveF=0.00363\n",
      "Adam: Epoch 1500 ObjectiveF=0.00277\n",
      "Adam: Epoch 2000 ObjectiveF=0.00142\n",
      "Adam: Epoch 2500 ObjectiveF=0.00064\n",
      "Adam: Epoch 3000 ObjectiveF=0.00040\n",
      "Adam: Epoch 3500 ObjectiveF=0.00037\n",
      "Adam: Epoch 4000 ObjectiveF=0.00035\n",
      "Adam: Epoch 4500 ObjectiveF=0.00033\n",
      "Adam: Epoch 5000 ObjectiveF=0.01179\n",
      "Net 6 RMSE 0.01113\n",
      "Adam: Epoch 500 ObjectiveF=0.18453\n",
      "Adam: Epoch 1000 ObjectiveF=0.06502\n",
      "Adam: Epoch 1500 ObjectiveF=0.39966\n",
      "Adam: Epoch 2000 ObjectiveF=0.39932\n",
      "Adam: Epoch 2500 ObjectiveF=0.39913\n",
      "Adam: Epoch 3000 ObjectiveF=0.39905\n",
      "Adam: Epoch 3500 ObjectiveF=0.39903\n",
      "Adam: Epoch 4000 ObjectiveF=0.39902\n",
      "Adam: Epoch 4500 ObjectiveF=0.39902\n",
      "Adam: Epoch 5000 ObjectiveF=0.39901\n",
      "Net 7 RMSE 0.32874\n",
      "Adam: Epoch 500 ObjectiveF=0.24664\n",
      "Adam: Epoch 1000 ObjectiveF=0.04515\n",
      "Adam: Epoch 1500 ObjectiveF=0.01559\n",
      "Adam: Epoch 2000 ObjectiveF=0.01031\n",
      "Adam: Epoch 2500 ObjectiveF=0.00961\n",
      "Adam: Epoch 3000 ObjectiveF=0.00902\n",
      "Adam: Epoch 3500 ObjectiveF=0.00817\n",
      "Adam: Epoch 4000 ObjectiveF=0.00757\n",
      "Adam: Epoch 4500 ObjectiveF=0.00680\n",
      "Adam: Epoch 5000 ObjectiveF=0.00603\n",
      "Net 8 RMSE 0.00484\n",
      "Adam: Epoch 500 ObjectiveF=0.07849\n",
      "Adam: Epoch 1000 ObjectiveF=0.01388\n",
      "Adam: Epoch 1500 ObjectiveF=0.00385\n",
      "Adam: Epoch 2000 ObjectiveF=0.00273\n",
      "Adam: Epoch 2500 ObjectiveF=0.00235\n",
      "Adam: Epoch 3000 ObjectiveF=0.00202\n",
      "Adam: Epoch 3500 ObjectiveF=0.00166\n",
      "Adam: Epoch 4000 ObjectiveF=0.00157\n",
      "Adam: Epoch 4500 ObjectiveF=0.00144\n",
      "Adam: Epoch 5000 ObjectiveF=0.00729\n",
      "Net 9 RMSE 0.00693\n",
      "Adam: Epoch 500 ObjectiveF=0.26009\n",
      "Adam: Epoch 1000 ObjectiveF=0.82843\n",
      "Adam: Epoch 1500 ObjectiveF=0.82842\n",
      "Adam: Epoch 2000 ObjectiveF=0.82842\n",
      "Adam: Epoch 2500 ObjectiveF=0.82842\n",
      "Adam: Epoch 3000 ObjectiveF=0.82842\n",
      "Adam: Epoch 3500 ObjectiveF=0.82842\n",
      "Adam: Epoch 4000 ObjectiveF=0.82842\n",
      "Adam: Epoch 4500 ObjectiveF=0.19088\n",
      "Adam: Epoch 5000 ObjectiveF=0.18054\n",
      "Net 10 RMSE 0.18008\n",
      "Adam: Epoch 500 ObjectiveF=0.03798\n",
      "Adam: Epoch 1000 ObjectiveF=0.00573\n",
      "Adam: Epoch 1500 ObjectiveF=0.00459\n",
      "Adam: Epoch 2000 ObjectiveF=0.00257\n",
      "Adam: Epoch 2500 ObjectiveF=0.00167\n",
      "Adam: Epoch 3000 ObjectiveF=0.00134\n",
      "Adam: Epoch 3500 ObjectiveF=0.00105\n",
      "Adam: Epoch 4000 ObjectiveF=0.00080\n",
      "Adam: Epoch 4500 ObjectiveF=0.00065\n",
      "Adam: Epoch 5000 ObjectiveF=0.00041\n",
      "Net 11 RMSE 0.00034\n",
      "Adam: Epoch 500 ObjectiveF=0.10411\n",
      "Adam: Epoch 1000 ObjectiveF=0.01023\n",
      "Adam: Epoch 1500 ObjectiveF=0.00934\n",
      "Adam: Epoch 2000 ObjectiveF=0.00694\n",
      "Adam: Epoch 2500 ObjectiveF=0.01259\n",
      "Adam: Epoch 3000 ObjectiveF=0.00282\n",
      "Adam: Epoch 3500 ObjectiveF=0.00280\n",
      "Adam: Epoch 4000 ObjectiveF=0.00520\n",
      "Adam: Epoch 4500 ObjectiveF=0.00190\n",
      "Adam: Epoch 5000 ObjectiveF=0.00166\n",
      "Net 12 RMSE 0.00126\n",
      "Adam: Epoch 500 ObjectiveF=0.15459\n",
      "Adam: Epoch 1000 ObjectiveF=0.07681\n",
      "Adam: Epoch 1500 ObjectiveF=0.07566\n",
      "Adam: Epoch 2000 ObjectiveF=0.07563\n",
      "Adam: Epoch 2500 ObjectiveF=0.07562\n",
      "Adam: Epoch 3000 ObjectiveF=0.07562\n",
      "Adam: Epoch 3500 ObjectiveF=0.07561\n",
      "Adam: Epoch 4000 ObjectiveF=0.07560\n",
      "Adam: Epoch 4500 ObjectiveF=0.07558\n",
      "Adam: Epoch 5000 ObjectiveF=0.07556\n",
      "Net 13 RMSE 0.07679\n",
      "Adam: Epoch 500 ObjectiveF=0.07855\n",
      "Adam: Epoch 1000 ObjectiveF=0.01699\n",
      "Adam: Epoch 1500 ObjectiveF=0.00550\n",
      "Adam: Epoch 2000 ObjectiveF=0.00364\n",
      "Adam: Epoch 2500 ObjectiveF=0.00228\n",
      "Adam: Epoch 3000 ObjectiveF=0.00151\n",
      "Adam: Epoch 3500 ObjectiveF=0.00106\n",
      "Adam: Epoch 4000 ObjectiveF=0.00080\n",
      "Adam: Epoch 4500 ObjectiveF=0.00065\n",
      "Adam: Epoch 5000 ObjectiveF=0.00056\n",
      "Net 14 RMSE 0.00071\n",
      "Adam: Epoch 500 ObjectiveF=0.22547\n",
      "Adam: Epoch 1000 ObjectiveF=0.03677\n",
      "Adam: Epoch 1500 ObjectiveF=0.00868\n",
      "Adam: Epoch 2000 ObjectiveF=0.00589\n",
      "Adam: Epoch 2500 ObjectiveF=0.00441\n",
      "Adam: Epoch 3000 ObjectiveF=0.00360\n",
      "Adam: Epoch 3500 ObjectiveF=0.00304\n",
      "Adam: Epoch 4000 ObjectiveF=0.00246\n",
      "Adam: Epoch 4500 ObjectiveF=0.00208\n",
      "Adam: Epoch 5000 ObjectiveF=0.00163\n",
      "Net 15 RMSE 0.00165\n",
      "Adam: Epoch 500 ObjectiveF=0.24001\n",
      "Adam: Epoch 1000 ObjectiveF=0.06812\n",
      "Adam: Epoch 1500 ObjectiveF=0.01912\n",
      "Adam: Epoch 2000 ObjectiveF=0.00431\n",
      "Adam: Epoch 2500 ObjectiveF=0.00322\n",
      "Adam: Epoch 3000 ObjectiveF=0.00260\n",
      "Adam: Epoch 3500 ObjectiveF=0.00209\n",
      "Adam: Epoch 4000 ObjectiveF=0.00152\n",
      "Adam: Epoch 4500 ObjectiveF=0.00338\n",
      "Adam: Epoch 5000 ObjectiveF=0.00374\n",
      "Net 16 RMSE 0.00293\n",
      "Adam: Epoch 500 ObjectiveF=0.09014\n",
      "Adam: Epoch 1000 ObjectiveF=0.04099\n",
      "Adam: Epoch 1500 ObjectiveF=0.01666\n",
      "Adam: Epoch 2000 ObjectiveF=0.00318\n",
      "Adam: Epoch 2500 ObjectiveF=0.00499\n",
      "Adam: Epoch 3000 ObjectiveF=0.00209\n",
      "Adam: Epoch 3500 ObjectiveF=0.00199\n",
      "Adam: Epoch 4000 ObjectiveF=0.00187\n",
      "Adam: Epoch 4500 ObjectiveF=0.00179\n",
      "Adam: Epoch 5000 ObjectiveF=0.00142\n",
      "Net 17 RMSE 0.00146\n",
      "Adam: Epoch 500 ObjectiveF=0.02823\n",
      "Adam: Epoch 1000 ObjectiveF=0.01342\n",
      "Adam: Epoch 1500 ObjectiveF=0.00723\n",
      "Adam: Epoch 2000 ObjectiveF=0.00470\n",
      "Adam: Epoch 2500 ObjectiveF=0.00329\n",
      "Adam: Epoch 3000 ObjectiveF=0.00224\n",
      "Adam: Epoch 3500 ObjectiveF=0.00152\n",
      "Adam: Epoch 4000 ObjectiveF=0.00109\n",
      "Adam: Epoch 4500 ObjectiveF=0.00072\n",
      "Adam: Epoch 5000 ObjectiveF=0.00065\n",
      "Net 18 RMSE 0.00069\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adam: Epoch 500 ObjectiveF=0.13717\n",
      "Adam: Epoch 1000 ObjectiveF=0.09200\n",
      "Adam: Epoch 1500 ObjectiveF=0.05447\n",
      "Adam: Epoch 2000 ObjectiveF=0.05254\n",
      "Adam: Epoch 2500 ObjectiveF=0.04954\n",
      "Adam: Epoch 3000 ObjectiveF=0.04323\n",
      "Adam: Epoch 3500 ObjectiveF=0.03099\n",
      "Adam: Epoch 4000 ObjectiveF=0.01746\n",
      "Adam: Epoch 4500 ObjectiveF=0.00728\n",
      "Adam: Epoch 5000 ObjectiveF=0.00261\n",
      "Net 19 RMSE 0.00195\n",
      "Adam: Epoch 500 ObjectiveF=0.14358\n",
      "Adam: Epoch 1000 ObjectiveF=0.00955\n",
      "Adam: Epoch 1500 ObjectiveF=0.00702\n",
      "Adam: Epoch 2000 ObjectiveF=0.00480\n",
      "Adam: Epoch 2500 ObjectiveF=0.00293\n",
      "Adam: Epoch 3000 ObjectiveF=0.00219\n",
      "Adam: Epoch 3500 ObjectiveF=0.00747\n",
      "Adam: Epoch 4000 ObjectiveF=0.01584\n",
      "Adam: Epoch 4500 ObjectiveF=0.39945\n",
      "Adam: Epoch 5000 ObjectiveF=0.39906\n",
      "Net 20 RMSE 0.32877\n",
      "0.04791906172653846\n",
      "\n",
      "--- 40/40 points. Returned correct value.\n",
      "\n",
      "======================================================================\n",
      "HW Execution Grade is 80 / 80\n",
      "======================================================================\n",
      "\n",
      " __ / 20 Results and discussion for Boston housing data.\n",
      "\n",
      "======================================================================\n",
      "HW FINAL GRADE is  _  / 100\n",
      "======================================================================\n",
      "\n",
      "Extra Credit:\n",
      "\n",
      "Apply your functions to a data set from the UCI Machine Learning Repository.\n",
      "Explain your steps and results in markdown cells.\n",
      "\n",
      "\n",
      "HW EXTRA CREDIT is 0 / 1\n"
     ]
    }
   ],
   "source": [
    "%run -i A2grader.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check-In\n",
    "\n",
    "Do not include this section in your notebook.\n",
    "\n",
    "Name your notebook ```Lastname-A2.ipynb```.  So, for me it would be ```Anderson-A2.ipynb```.  Submit the file using the ```Assignment 2``` link on [Canvas](https://colostate.instructure.com/courses/131494)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra Credit\n",
    "\n",
    "Apply your multilayer neural network code to a regression problem using data that you choose \n",
    "from the [UCI Machine Learning Repository](http://archive.ics.uci.edu/ml/datasets.php). Pick a dataset that\n",
    "is listed as being appropriate for regression."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "512px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
