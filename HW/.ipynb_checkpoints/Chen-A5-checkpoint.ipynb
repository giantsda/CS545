{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A5.1 Reinforcement Learning for Marble with Variable Goal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this assignment, start with the `19 Reinforcement Learning Modular Framework` notebook.  Recall that this code used reinforcement learning to learn to push a marble towards the goal position of 5 and keep it there.\n",
    "\n",
    "The objective of the following required modification is an agent that has been trained to directly move the marble to a specified goal without any further training. \n",
    "\n",
    "<font color=\"red\">Modify the code</font> to allow any goal position from 1 to 9.  First, rename the `Marble` class to `Marble_Variable_Goal`.  Then, modify the `Marble_Variable_Goal` class so that it includes the goal in the state, allowing the agent to learn to push the marble to any given goal.  Modify the `intial_state` function to set the goal to a random integer from 1 to 9.\n",
    "\n",
    "<font color='red'>Do not modify</font> the `Qnet` class. It should run correctly when applied to your new `Marble_Variable_Goal` class.\n",
    "\n",
    "<font color='red'>Discuss</font> what you modified in the code for this assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>Add some code</font> at the end of the notebook that applies the trained agent to the marble at goals from 1 to 9.  For each goal, start the marble at positions 0 through 10 with zero velocity and the specified goal and applies the trained agent to control the marble for 200 steps.  Calculate the distance of the final state from the goal.  Average this distance over all starting positions for the specified goal and store in a numpy array with one row for each goal and each row containing the goal and the average of distances to goal over all starting positions. Call this numpy array, `distances_to_goal`.  Plot the results of these average distances versus the goal.\n",
    "\n",
    "<font color='red'>Explore different parameter values</font>, including the network hidden layer structure, number of trials, number of steps per trial, learning rate, number of epochs, and final epsilon value to try to get the best results for `distances_to_goal`. Try just three or four different values for each parameter, varying one parameter value at a time. After you have found some parameter values that often work well, set the parameters to these values and run again to produce the graphs from `plot_status` showing the results with these parameters. But, first <font color='red'>modify `plot_status` code</font> for subplots 6 and 9 so that the vertical pink goal region correctly shows the current goal.  Add the current goal to the title of the subplot 9.\n",
    "\n",
    "<font color='red'>Discuss</font> the results, and discuss which parameter values seem to perform well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is some code and parameter values that I have found to be successful...usually.  As you know, results vary quite a bit from one run to another. Understand that you will not find parameter values that work perfectly every time.  You are welcome to start with these parameter values and experiment with variations of these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import time  # for sleep\n",
    "import IPython.display as ipd  # for display and clear_output\n",
    "from IPython.display import display, clear_output  # for the following animation\n",
    "import os\n",
    "import copy\n",
    "import signal\n",
    "import os\n",
    "import numpy as np\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.colors import LightSource\n",
    "import optimizers as opt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    " \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import neuralnetworks_A4 as nn   # from A4\n",
    "\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "    \n",
    "class Environment(ABC):\n",
    "    \n",
    "    def __init__(self, valid_actions):\n",
    "        self.valid_actions = valid_actions\n",
    "\n",
    "    @abstractmethod\n",
    "    def initial_state(self):\n",
    "        return state  # the initial state\n",
    "    \n",
    "    @abstractmethod\n",
    "    def next_state(self, state, action):\n",
    "        return next_state  \n",
    "    \n",
    "    @abstractmethod\n",
    "    def reinforcement(self, state):\n",
    "        return r # scalar reinforcement\n",
    "   \n",
    "    def terminal_state(self, state):\n",
    "        return False  # True if state is terminal state\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Agent(ABC):\n",
    "    \n",
    "    def __init__(self, environment):\n",
    "        self.environment = environment\n",
    "\n",
    "    @abstractmethod\n",
    "    def make_samples(self, n_samples, epsilon):\n",
    "        return X, R, Qn, terminal_state\n",
    "\n",
    "    def update_Qn(self, X, Qn, terminal_state):\n",
    "        n_samples = X.shape[0]\n",
    "        for i in range(n_samples - 1):\n",
    "            if not terminal_state[i+1]:\n",
    "                Qn[i] = self.use(X[i+1])\n",
    "        return Qn\n",
    "\n",
    "    def epsilon_greedy(self, state, epsilon):\n",
    "        valid_actions = self.environment.valid_actions\n",
    "\n",
    "        if np.random.uniform() < epsilon:\n",
    "            # Random Move\n",
    "            action = np.random.choice(valid_actions)\n",
    "        else:\n",
    "            # Greedy Move\n",
    "            Qs = [self.use(np.hstack((state, a)).reshape((1, -1))) for a in valid_actions]\n",
    "            ai = np.argmax(Qs)\n",
    "            action = valid_actions[ai]\n",
    "\n",
    "        return action\n",
    "\n",
    "    @abstractmethod\n",
    "    def train(self):\n",
    "        return\n",
    "\n",
    "    @abstractmethod\n",
    "    def use(self, X):\n",
    "        return # Q values for each row of X, each consisting of state and action\n",
    "class Qnet(Agent):\n",
    "\n",
    "    def __init__(self, environment, hidden_layers, X_means=None, X_stds=None, Q_means=None, Q_stds=None):\n",
    "        self.environment = environment\n",
    "        state_size = environment.initial_state().size  # assumes state is an np.array\n",
    "        valid_actions = environment.valid_actions\n",
    "        action_size = 1 if valid_actions.ndim == 1 else valid_actions.shape[1]\n",
    "\n",
    "        self.Qnet = nn.NeuralNetwork(state_size + action_size, hidden_layers, 1)\n",
    "        if X_means:\n",
    "            self.Qnet.X_means = np.array(X_means)\n",
    "            self.Qnet.X_stds = np.array(X_stds)\n",
    "            self.Qnet.T_means = np.array(Q_means)\n",
    "            self.Qnet.T_stds = np.array(Q_stds)\n",
    "\n",
    "    def make_samples(self, n_samples, epsilon):\n",
    " \n",
    "        state_size = self.environment.initial_state().size  # assumes state is an np.array\n",
    "        valid_actions = self.environment.valid_actions\n",
    "        action_size = 1 if valid_actions.ndim == 1 else valid_actions.shape[1]\n",
    "\n",
    "        X = np.zeros((n_samples, state_size + action_size))\n",
    "        R = np.zeros((n_samples, 1))\n",
    "        Qn = np.zeros((n_samples, 1))\n",
    "        terminal_state = np.zeros((n_samples, 1), dtype=bool)  # All False values\n",
    "\n",
    "        state = self.environment.initial_state()\n",
    "        state = self.environment.next_state(state, 0)        # Update state, sn from s and a\n",
    "        action = self.epsilon_greedy(state, epsilon)\n",
    "\n",
    "        # Collect data from n_samples steps\n",
    "        for step in range(n_samples):\n",
    "\n",
    "            next_state = self.environment.next_state(state, action)        # Update state, sn from s and a\n",
    "            r = self.environment.reinforcement(state)   # Calculate resulting reinforcement\n",
    "            next_action = self.epsilon_greedy(next_state, epsilon)\n",
    "            X[step, :] = np.hstack((state, action))\n",
    "            R[step, 0] = r\n",
    "            if self.environment.terminal_state(state):\n",
    "                terminal_state[step, 0] = True\n",
    "                Qn[step, 0] = 0\n",
    "            else:\n",
    "                Qn[step, 0] = self.use(np.hstack((next_state, next_action)))\n",
    "            # Advance one time step\n",
    "            state, action = next_state, next_action\n",
    "\n",
    "        return X, R, Qn, terminal_state\n",
    "\n",
    "    def update_Qn(self, X, Qn, terminal_state):\n",
    "        n_samples = X.shape[0]\n",
    "        for i in range(n_samples - 1):\n",
    "            if not terminal_state[i+1]:\n",
    "                Qn[i] = self.use(X[i+1])\n",
    "        return Qn\n",
    "\n",
    "    def train(self, n_trials, n_steps_per_trial, n_epochs, method, learning_rate, \n",
    "              gamma, epsilon, final_epsilon,\n",
    "              trial_callback=None):\n",
    "\n",
    "        if trial_callback:\n",
    "            fig = plt.figure(figsize=(10, 10))\n",
    "            \n",
    "        epsilon_decay =  np.exp(np.log(final_epsilon) / n_trials) # to produce this final value\n",
    "        print('epsilon_decay is', epsilon_decay)\n",
    "        epsilon_trace = np.zeros(n_trials)\n",
    "        r_trace = np.zeros(n_trials)\n",
    "\n",
    "        for trial in range(n_trials):\n",
    "\n",
    "            X, R, Qn, terminal_state = self.make_samples(n_steps_per_trial, epsilon)\n",
    "\n",
    "            for epoch in range(n_epochs):\n",
    "                self.Qnet.train(X, R + gamma * Qn, 1,  method=method, learning_rate=learning_rate, batch_size=-1, verbose=False)\n",
    "                self.update_Qn(X, Qn, terminal_state)\n",
    "\n",
    "            epsilon *= epsilon_decay\n",
    "\n",
    "            # Rest is for plotting\n",
    "            epsilon_trace[trial] = epsilon\n",
    "            r_trace[trial] = np.mean(R)\n",
    "\n",
    "#             if trial_callback and (trial + 1 == n_trials or trial % (n_trials / 10) == 0):\n",
    "#                 print('runing %2.2f' % (trial / (n_trials / 10))*100)\n",
    "#                 fig.clf()\n",
    "#                 trial_callback(agent, trial, n_trials, X, epsilon_trace, r_trace)\n",
    "#                 clear_output(wait=True)\n",
    "#                 display(fig)\n",
    "\n",
    "        if trial_callback:\n",
    "            clear_output(wait=True)\n",
    "\n",
    "        return epsilon_trace, r_trace\n",
    "\n",
    "\n",
    "    def use(self, X):\n",
    "        return self.Qnet.use(X)\n",
    "from matplotlib import cm\n",
    "\n",
    "def plot_status(agent, trial, n_trials, X, epsilon_trace, r_trace):\n",
    "    a=1\n",
    "\n",
    "# =============================================================================\n",
    "#   plt.subplot(3, 3, 1)\n",
    "#   plt.plot(epsilon_trace[:trial + 1])\n",
    "#   plt.ylabel('Random Action Probability ($\\epsilon$)')\n",
    "#   plt.ylim(0, 1)\n",
    "#              \n",
    "#   plt.subplot(3, 3, 2)\n",
    "#   plt.plot(r_trace[:trial + 1], alpha=0.5)\n",
    "#   plt.ylabel('Mean reinforcement')\n",
    "#       \n",
    "#   valid_actions = agent.environment.valid_actions\n",
    "# \n",
    "#   qs = agent.use(np.array([[s, 0, a] for a in valid_actions for s in range(11)]))\n",
    "#              \n",
    "#   plt.subplot(3, 3, 3)\n",
    "#   acts = ['L', '0', 'R']\n",
    "#   actsiByState = np.argmax(qs.reshape((len(valid_actions), -1)), axis=0)\n",
    "#   for i in range(11):\n",
    "#       plt.text(i, 0, acts[actsiByState[i]])\n",
    "#       plt.xlim(-1, 11)\n",
    "#       plt.ylim(-1, 1)\n",
    "#   plt.text(2, 0.2,'Policy for Zero Velocity')\n",
    "#   plt.axis('off')\n",
    "#              \n",
    "#              \n",
    "#   plt.subplot(3, 3, 4)\n",
    "#   n = 20\n",
    "#   positions = np.linspace(0, 10, n)\n",
    "#   velocities =  np.linspace(-5, 5, n)\n",
    "#   xs, ys = np.meshgrid(positions, velocities)\n",
    "#   xsflat = xs.flat\n",
    "#   ysflat = ys.flat\n",
    "#   qs = agent.use(np.array([[xsflat[i], ysflat[i], a] for a in valid_actions for i in range(len(xsflat))]))\n",
    "#   qs = qs.reshape((len(valid_actions), -1)).T\n",
    "#   qsmax = np.max(qs, axis=1).reshape(xs.shape)\n",
    "#   cs = plt.contourf(xs, ys, qsmax, 20, cmap=cm.coolwarm)\n",
    "#   plt.colorbar(cs)\n",
    "#   plt.xlabel('$x$')\n",
    "#   plt.ylabel('$\\dot{x}$')\n",
    "#   plt.title('Max Q')\n",
    "#              \n",
    "#   plt.subplot(3, 3, 5)\n",
    "#   acts = np.array(valid_actions)[np.argmax(qs, axis=1)].reshape(xs.shape)\n",
    "#   cs = plt.contourf(xs, ys, acts, [-2, -0.5, 0.5, 2], cmap=cm.coolwarm)\n",
    "#   plt.colorbar(cs)\n",
    "#   plt.xlabel('$x$')\n",
    "#   plt.ylabel('$\\dot{x}$')\n",
    "#   plt.title('Actions')\n",
    "#   \n",
    "#   plt.subplot(3, 3, 6)\n",
    "#   plt.plot(X[:, 0], X[: ,1])\n",
    "#   plt.plot(X[-1, 0], X[-1, 1], 'ro')\n",
    "#   plt.xlabel('$x$')\n",
    "#   plt.ylabel('$\\dot{x}$')\n",
    "#   plt.fill_between([4, 6], [-5, -5], [5, 5], color='red', alpha=0.3)  # CHECK OUT THIS FUNCTION!\n",
    "#   plt.xlim(-1, 11)\n",
    "#   plt.ylim(-5, 5)\n",
    "#   plt.title('Last Trial')\n",
    "# \n",
    "#   ax = plt.subplot(3, 3, 7, projection='3d')\n",
    "#   ax.plot_surface(xs, ys, qsmax, linewidth=0, cmap=cm.coolwarm)\n",
    "#   ax.set_xlabel('$x$')\n",
    "#   ax.set_ylabel('$\\dot{x}$')\n",
    "#   plt.title('Max Q')\n",
    "#   \n",
    "#   ax = plt.subplot(3, 3, 8, projection='3d')\n",
    "#   ax.plot_surface(xs, ys, acts, cmap=cm.coolwarm, linewidth=0)\n",
    "#   ax.set_xlabel('$x$')\n",
    "#   ax.set_ylabel('$\\dot{x}$')\n",
    "#   plt.title('Action')\n",
    "#   \n",
    "#   test_it(agent, 10, 500)\n",
    "# \n",
    "#   plt.tight_layout()\n",
    "# =============================================================================\n",
    "\n",
    "def test_it(agent, n_trials, n_steps_per_trial):\n",
    "    xs = np.linspace(0, 10, n_trials)\n",
    "    plt.subplot(3, 3, 9) \n",
    "    \n",
    "    # For a number (n_trials) of starting positions, run marble sim for n_steps_per_trial\n",
    "    for x in xs:\n",
    "        \n",
    "        s = [x, 0]  # 0 velocity\n",
    "        x_trace = np.zeros((n_steps_per_trial, 2))\n",
    "        for step in range(n_steps_per_trial):\n",
    "            a = agent.epsilon_greedy(s, 0.0) # epsilon = 0\n",
    "            s = agent.environment.next_state(s, a)\n",
    "            x_trace[step, :] = s\n",
    "            \n",
    "        plt.plot(x_trace[:, 0], x_trace[:, 1])\n",
    "        plt.plot(x_trace[-1, 0], x_trace[-1, 1], 'ro')\n",
    "        plt.fill_between([4, 6], [-5, -5], [5, 5], color='pink', alpha=0.3)\n",
    "        plt.xlim(-1, 11)\n",
    "        plt.ylim(-5, 5)\n",
    "        plt.ylabel('$\\dot{x}$')\n",
    "        plt.xlabel('$x$')\n",
    "        plt.title('State Trajectories for $\\epsilon=0$')\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "class Marble_Variable_Goal(Environment):\n",
    "\n",
    "    def __init__(self, valid_actions):\n",
    "        super().__init__(valid_actions)\n",
    "        self.goal = None\n",
    "        \n",
    "        \n",
    "    def initial_state(self):\n",
    "        goal=np.random.randint(1,10)\n",
    "        self.goal =goal\n",
    "        return np.array([10 * np.random.uniform(), 0.0, goal])\n",
    "\n",
    "    def next_state(self, state, action):\n",
    "        '''[0] is position, s[1] is velocity. a is -1, 0 or 1'''    \n",
    "        next_state = state.copy()\n",
    "        deltaT = 0.1                           # Euler integration time step\n",
    "        next_state[0] += deltaT * state[1]                  # Update position\n",
    "        force = action\n",
    "        mass = 0.5\n",
    "        next_state[1] += deltaT * (force / mass - 0.2 * state[1])  # Update velocity. Includes friction\n",
    "        \n",
    "        next_state[2]=self.goal \n",
    "        # Bound next position. If at limits, set velocity to 0.\n",
    "        if next_state[0] < 0:        \n",
    "            next_state = [0., 0.,self.goal ]    # these constants as ints were causing the errors we discussed in class. I DON'T KNOW WHY!!\n",
    "        elif next_state[0] > 10:\n",
    "            next_state = [10., 0.,self.goal ]\n",
    "\n",
    "        return next_state\n",
    "\n",
    "    def reinforcement(self, state):\n",
    "        goal = self.goal\n",
    "        return 0 if abs(state[0]- goal) < 1 else -1\n",
    "\n",
    "    def terminal_state(self, state):\n",
    "        return False\n",
    "    def changeGoal(self,newgoal):\n",
    "        self.goal=newgoal\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "def distances_to_goal(agent,goal):\n",
    "    n_steps_per_trial=200\n",
    "    xs = np.linspace(0, 10, 25)\n",
    "    d=[]\n",
    "    for x in xs:\n",
    "        s = [0, 0, goal]  # 0 velocity\n",
    "        x_trace = np.zeros((n_steps_per_trial, 3))\n",
    "        for step in range(n_steps_per_trial):\n",
    "            a = agent.epsilon_greedy(s, 0.0) # epsilon = 0\n",
    "            s = agent.environment.next_state(s, a)\n",
    "            x_trace[step, :] = s\n",
    "        lastPostion=x_trace[-1,0]\n",
    "        d.append(abs(lastPostion-goal))\n",
    " \n",
    "    return sum(d)/len(d)  # return average distance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 720x720 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "marble = Marble_Variable_Goal(valid_actions=np.array([-1, 0, 1]))\n",
    "\n",
    "agent = Qnet(marble, hidden_layers=[10, 10],\n",
    "             X_means=[5, 0, 5, 0], X_stds=[2, 2, 2, 0.8],\n",
    "             Q_means=[-2], Q_stds=[1])\n",
    "\n",
    "epsilon_trace, r_trace =  agent.train(n_trials=50, n_steps_per_trial=20, n_epochs=10,\n",
    "                                      method='sgd', learning_rate=0.01, gamma=0.9,\n",
    "                                      epsilon=1, final_epsilon=0.01,\n",
    "                                      trial_callback=plot_status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "marble = Marble_Variable_Goal(valid_actions=np.array([-1, 0, 1]))\n",
    " \n",
    "goals=[]\n",
    "distances=[]\n",
    "for goal in range(1,10):\n",
    "    marble.changeGoal(goal)\n",
    "    agent = Qnet(marble, hidden_layers=[10, 10],\n",
    "                 X_means=[5, 0, 5, 0], X_stds=[2, 2, 2, 0.8],\n",
    "                 Q_means=[-2], Q_stds=[1])\n",
    " \n",
    "    epsilon_trace, r_trace =  agent.train(n_trials=300, n_steps_per_trial=200, n_epochs=100,\n",
    "                                          method='sgd', learning_rate=0.01, gamma=0.9,\n",
    "                                          epsilon=1, final_epsilon=0.01,\n",
    "                                          trial_callback=plot_status)\n",
    "    d=distances_to_goal(agent,goal)\n",
    "    goals.append(goal)\n",
    "    distances.append(d)\n",
    "\n",
    "    print(f'goal={goal}; distance={d}\\n')\n",
    "plt.plot(goals,distances,'o-')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epsilon_decay is 0.9697653591082493\n"
     ]
    }
   ],
   "source": [
    "marble = Marble_Variable_Goal(valid_actions=np.array([-1, 0, 1]))\n",
    "goal=5\n",
    "n_trialsS=(30, 100, 150)\n",
    "ds=[]\n",
    "for n_trials in n_trialsS:  \n",
    "    marble.changeGoal(goal)\n",
    "    agent = Qnet(marble, hidden_layers=[10, 10],\n",
    "                 X_means=[5, 0, 5, 0], X_stds=[2, 2, 2, 0.8],\n",
    "                 Q_means=[-2], Q_stds=[1])\n",
    "\n",
    "    epsilon_trace, r_trace =  agent.train(n_trials=n_trials, n_steps_per_trial=100, n_epochs=150,\n",
    "                                          method='sgd', learning_rate=0.01, gamma=0.9,\n",
    "                                          epsilon=1, final_epsilon=0.01,\n",
    "                                          trial_callback=plot_status)\n",
    "    d=distances_to_goal(agent,goal)\n",
    "    ds.append(d)\n",
    "\n",
    "for i in range(0,3):\n",
    "    print(f'n_trials={n_trialsS[i]}; distance={ds[i]}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "marble = Marble_Variable_Goal(valid_actions=np.array([-1, 0, 1]))\n",
    "goal=5\n",
    "hidden_layersS=([0],[3,3],[7,7])\n",
    "ds=[]\n",
    "for hidden_layers in hidden_layersS:  \n",
    "    marble.changeGoal(goal)\n",
    "    agent = Qnet(marble, hidden_layers=hidden_layers,\n",
    "                 X_means=[5, 0, 5, 0], X_stds=[2, 2, 2, 0.8],\n",
    "                 Q_means=[-2], Q_stds=[1])\n",
    "\n",
    "    epsilon_trace, r_trace =  agent.train(n_trials=150, n_steps_per_trial=100, n_epochs=150,\n",
    "                                          method='sgd', learning_rate=0.01, gamma=0.9,\n",
    "                                          epsilon=1, final_epsilon=0.01,\n",
    "                                          trial_callback=plot_status)\n",
    "    d=distances_to_goal(agent,goal)\n",
    "    ds.append(d)\n",
    "\n",
    "for i in range(0,3):\n",
    "    print(f'hidden_layers={hidden_layersS[i]}; distance={ds[i]}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "marble = Marble_Variable_Goal(valid_actions=np.array([-1, 0, 1]))\n",
    "goal=5\n",
    "n_steps_per_trialS=(10, 50, 100)\n",
    "ds=[]\n",
    "for n_steps_per_trial in n_steps_per_trialS:  \n",
    "    marble.changeGoal(goal)\n",
    "    agent = Qnet(marble, hidden_layers=[10, 10],\n",
    "                 X_means=[5, 0, 5, 0], X_stds=[2, 2, 2, 0.8],\n",
    "                 Q_means=[-2], Q_stds=[1])\n",
    "\n",
    "    epsilon_trace, r_trace =  agent.train(n_trials=150, n_steps_per_trial=100, n_epochs=50,\n",
    "                                          method='sgd', learning_rate=0.01, gamma=0.9,\n",
    "                                          epsilon=1, final_epsilon=0.01,\n",
    "                                          trial_callback=plot_status)\n",
    "    d=distances_to_goal(agent,goal)\n",
    "    ds.append(d)\n",
    "\n",
    "for i in range(0,3):\n",
    "    print(f'n_trials={n_steps_per_trialS[i]}; distance={ds[i]}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grading\n",
    "\n",
    "Download [A5grader.tar](https://www.cs.colostate.edu/~anderson/cs545/notebooks/A5grader.tar) and extract `A5grader.py` before running the following cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-01T15:36:29.411400Z",
     "start_time": "2021-11-01T15:36:25.333723Z"
    }
   },
   "outputs": [],
   "source": [
    "%run -i A5grader.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Credit\n",
    "\n",
    "Receive 1 point of extra credit for each of these:\n",
    "\n",
    "   * Modify your solution to this assignment by creating and using a `Marble2D` class that simulates the marble moving in two-dimensions, on a plane.  Some of the current plots will not work for this case. Just show the ones that are still appropriate.\n",
    "   * Experiment with seven valid actions rather than three.  How does this change the behavior of the controlled marble?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
